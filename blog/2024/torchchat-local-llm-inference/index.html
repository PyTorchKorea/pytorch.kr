<!DOCTYPE html>
<html lang="ko">
<head>
<!-- Google Tag Manager -->
<script data-cfasync="false">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-57L6X5C');</script>
<!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      토치챗(torchchat) 소개: 노트북, 데스크탑 및 모바일에서 로컬 LLM 추론 가속화하기 | 파이토치 한국 사용자 모임
    
  </title>
  <meta property="og:title" content="파이토치 한국 사용자 모임 (PyTorch Korea User Group)" />
<meta
  name="description"
  property="og:description"
  content="파이토치 한국 사용자 모임에 오신 것을 환영합니다. 딥러닝 프레임워크인 파이토치(PyTorch)를 사용하는 한국어 사용자들을 위해 문서를 번역하고 정보를 공유하고 있습니다."
/>
<meta property="og:url" content="https://pytorch.kr" />
<meta property="og:type" content="website" />
<meta
  property="og:image"
  content="https://pytorch.kr/assets/images/pytorch-kr-logo.png"
/>
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript data-cfasync="false"><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L6X5C"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.kr" aria-label="PyTorchKR"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">시작하기</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">블로그</a>
    </li>

    <li class="main-menu-item">
      <a href="https://tutorials.pytorch.kr/" target="_self">튜토리얼</a>
    </li>

    <li class="main-menu-item ">
      <a href="/hub">허브</a>
    </li>

    <li class="main-menu-item">
      <a href="https://discuss.pytorch.kr/" target="_self">커뮤니티</a>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">July 30, 2024</p>
            <h1><a class="blog-title">토치챗(torchchat) 소개: 노트북, 데스크탑 및 모바일에서 로컬 LLM 추론 가속화하기</a></h1>
            
                <h4 class="blog-subtitle">Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile</h4>
            
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      
                        
                            작성: PyTorch Team, 번역: PyTorch Korea User Group
                        
                      
                    </p>
                    
                    <p class="translation-description">
                        이 글은 <a href="https://pytorch.org/blog/">PyTorch 공식 블로그</a>의 글을 번역한 것입니다.
                        번역 글은 원문을 함께 표시합니다. 원문을 읽으시려면 <a href="https://pytorch.org/blog/torchchat-local-llm-inference/">여기</a>를 클릭하세요.
                        (This article is a Korean translation of the original post on the <a href="https://pytorch.org/blog/">PyTorch official blog</a>.
                        Below translation includes the original text. To read the original post, click <a href="https://pytorch.org/blog/torchchat-local-llm-inference/">here</a>.)
                    </p>
                    
                    <div class="blog-content">
                        <p>오늘 노트북과 데스크탑, 모바일에서 Llama 3와 3.1, 그리고 다른 대규모 언어 모델(LLM, Large Language Model)을 원활하고 고성능으로 실행하는 방법을 보여주는 라이브러리인 <a href="https://github.com/pytorch/torchchat">torchchat</a>을 출시했습니다.</p>
<blockquote>
  <p>Today, we’re releasing <a href="https://github.com/pytorch/torchchat">torchchat</a>, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.</p>
</blockquote>

<p>이전 블로그 게시물에서는 네이티브 PyTorch 2에서 CUDA를 활용하여 LLM을 뛰어난 성능으로 실행하는 것을 <a href="https://pytorch.org/blog/accelerating-generative-ai-2/">보여드렸었습니다</a>. Torchchat은 이를 더 많은 대상 환경과 모델, 실행 모드에서 확장했습니다. 또한, 내보내기(export)나 양자화(quantization), 평가(eval)와 같은 주요 기능들을 이해하기 쉬운 방식으로 제공하여 로컬 추론 솔루션을 구축하려는 사람들에게 시작부터 끝까지 알려(E2E story)드립니다.</p>
<blockquote>
  <p>In our previous blog posts, we <a href="https://pytorch.org/blog/accelerating-generative-ai-2/">showed</a> how to use native PyTorch 2 to run LLMs with great performance using CUDA. Torchchat expands on this with more target environments, models and execution modes. Additionally it provides important functions such as export, quantization and eval in a way that’s easy to understand providing an E2E story for those who want to build a local inference solution.</p>
</blockquote>

<p>Torchchat은 3가지 영역으로 구성되어 있습니다:</p>
<blockquote>
  <p>You will find the project organized into three areas:</p>
</blockquote>

<ul>
  <li>Python: Torchchat은 Python CLI를 통해 호춣하거나 브라우저로 접근할 수 있는 <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#server">REST API</a>를 제공합니다.
    <blockquote>
      <ul>
        <li>Python: Torchchat provides a <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#server">REST API</a> that is called via a Python CLI or can be accessed via the browser</li>
      </ul>
    </blockquote>
  </li>
  <li>C++: 파이토치(PyTorch)의 <a href="https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor">AOTInductor</a> 백엔드를 사용하여 데스크탑용 바이너리(desktop-friendly binary)를 생성합니다.
    <blockquote>
      <ul>
        <li>C++: Torchchat produces a desktop-friendly binary using PyTorch’s <a href="https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor">AOTInductor</a> backend</li>
      </ul>
    </blockquote>
  </li>
  <li>모바일 기기: Torchchat은 <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch</a> 를 사용하여 온디바이스 추론을 위한 .pte 바이너리 파일을 내보냅니다.
    <blockquote>
      <ul>
        <li>Mobile devices: Torchchat uses <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch</a> to export a .pte binary file for on-device inference</li>
      </ul>
    </blockquote>
  </li>
</ul>

<p><img src="/assets/images/torchchat.png" alt="토치챗 구조 / torchchat schema" style="width:100%" /></p>

<h2 id="성능--performance">성능 / Performance</h2>

<p>다음 표는 다양한 구성에서의 Llama 3의 torchchat 성능을 살펴봅니다.
<em>Llama 3.1에서의 수치들은 곧 공개 예정입니다.</em></p>
<blockquote>
  <p>The following table tracks the performance of torchchat for Llama 3 for a variety of configurations.<br />
<em>Numbers for Llama 3.1 are coming soon.</em></p>
</blockquote>

<p><strong>애플 맥북 M1 Max 64GB 노트북에서 Llama3 8B Instruct 성능 / Llama 3 8B Instruct on Apple MacBook Pro M1 Max 64GB Laptop</strong></p>

<table class="table table-bordered">
  <tr>
   <td><strong>Mode</strong>
   </td>
   <td><strong>DType</strong>
   </td>
   <td><strong>Llama 3 8B Tokens/Sec</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="3">Arm Compile
   </td>
   <td>float16
   </td>
   <td>5.84
   </td>
  </tr>
  <tr>
   <td>int8
   </td>
   <td>1.63
   </td>
  </tr>
  <tr>
   <td>int4
   </td>
   <td>3.99
   </td>
  </tr>
  <tr>
   <td rowspan="3">Arm AOTI
   </td>
   <td>float16
   </td>
   <td>4.05
   </td>
  </tr>
  <tr>
   <td>int8
   </td>
   <td>1.05
   </td>
  </tr>
  <tr>
   <td>int4
   </td>
   <td>3.28
   </td>
  </tr>
  <tr>
   <td rowspan="3">MPS Eager
   </td>
   <td>float16
   </td>
   <td>12.63
   </td>
  </tr>
  <tr>
   <td>int8
   </td>
   <td>16.9
   </td>
  </tr>
  <tr>
   <td>int4
   </td>
   <td>17.15
   </td>
  </tr>
</table>

<p><strong>CUDA 및 Linux x86에서 Llama3 8B Instruct 성능 / Llama 3 8B Instruct on Linux x86 and CUDA</strong><br />
<em>Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz with 180GB Ram + A100 (80GB)</em></p>

<table class="table table-bordered">
  <tr>
   <td>
<strong>Mode</strong>
   </td>
   <td><strong>DType</strong>
   </td>
   <td><strong>Llama 3 8B Tokens/Sec</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="3">x86 Compile
   </td>
   <td>bfloat16
   </td>
   <td>2.76
   </td>
  </tr>
  <tr>
   <td>int8
   </td>
   <td>3.15
   </td>
  </tr>
  <tr>
   <td>int4
   </td>
   <td>5.33
   </td>
  </tr>
  <tr>
   <td rowspan="3">CUDA Compile
   </td>
   <td>bfloat16
   </td>
   <td>83.23
   </td>
  </tr>
  <tr>
   <td>int8
   </td>
   <td>118.17
   </td>
  </tr>
  <tr>
   <td>int4
   </td>
   <td>135.16
   </td>
  </tr>
</table>

<p><strong>모바일에서 Llama3 8B Instruct 성능 / Llama3 8B Instruct on Mobile</strong><br />
Torchchat은 ExecuTorch를 통해 4-bit GPTQ를 사용하여 삼성 갤럭시 S23 및 아이폰에서 초당 8T 이상의 속도를 달성했습니다.</p>
<blockquote>
  <p>Torchchat achieves &gt; 8T/s on the Samsung Galaxy S23 and iPhone using 4-bit GPTQ via ExecuTorch.</p>
</blockquote>

<h2 id="결론--conclusion">결론 / Conclusion</h2>

<p><strong><a href="https://github.com/pytorch/torchchat">torchchat 저장소를 복제하여 사용해보시기</a></strong>를 권해드립니다. torchchat의 기능을 살펴보고, 파이토치 커뮤니티(PyTorch community)가 로컬 및 제약이 있는 기기(constrained device)에서 LLM을 실행할 수 있도록 역량을 강화하는 과정에 대한 여러분의 피드백을 공유해주시기 바랍니다. 생성형 AI와 LLM의 잠재력을 모든 기기에서 발휘할 수 있도록 함께 해주세요! 빠르게 반복하는 과정 중에 있으므로, 발견하시는 내용들은 <a href="https://github.com/pytorch/torchat/issues">이슈</a>로 남겨주세요. 또한, 모델 추가를 비롯하여 지원하는 하드웨어, 새로운 양자화 기법, 성능 개선 등의 광범위한 범위에 걸친 커뮤니티의 기여를 기다리고 있습니다. 즐거운 실험되시길 바랍니다!</p>
<blockquote>
  <p>We encourage you to <strong><a href="https://github.com/pytorch/torchchat">clone the torchchat repo and give it a spin</a></strong>, explore its capabilities, and share your feedback as we continue to empower the PyTorch community to run LLMs locally and on constrained devices. Together, let’s unlock the full potential of generative AI and LLMs on any device. Please submit <a href="https://github.com/pytorch/torchat/issues">issues</a> as you see them, since we are still iterating quickly. We’re also inviting community contributions across a broad range of areas, from additional models, target hardware support, new quantization schemes, or performance improvements.  Happy experimenting!</p>
</blockquote>

                    </div>
                    <hr noshade />
                    <p class="translation-description ad-discuss">
                         더 궁금하시거나 나누고 싶은 이야기가 있으신가요? <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티에 참여</a>해주세요!
                    </p>
                </article>
            </div>
        </div>
    </div>

    <div class="blog-comment">
  <div class="container">
      <div id="discourse-comments"></div>
      <meta name="discourse-username" content="bot">
  </div>
</div>

<script type="text/javascript">
  DiscourseEmbed = {
      discourseUrl: 'https://discuss.pytorch.kr/',
      discourseEmbedUrl: 'https://pytorch.kr/blog/2024/torchchat-local-llm-inference/',
      // className: 'CLASS_NAME',
  };

  (function() {
      var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
      d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>

    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="">파이토치 한국 사용자 모임</a></li>
          <li><a href="/about">사용자 모임 소개</a></li>
          <li><a href="/contributors">기여해주신 분들</a></li>
          <li><a href="/resources">리소스</a></li>
          <li><a href="/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LEHG248408"></script>
<script data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LEHG248408');   // GA4
</script>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.kr" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>

        <li class="main-menu-item ">
          <a href="/get-started">시작하기</a>
        </li>

        <li class="main-menu-item active">
          <a href="/blog">블로그</a>
        </li>

        <li class="main-menu-item">
          <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
        </li>

        <li class="main-menu-item">
          <a href="/hub">허브</a>
        </li>

        <li class="main-menu-item">
          <a href="https://discuss.pytorch.kr/">커뮤니티</a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>
<!-- 
  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script> -->

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .coc-header, .announcement-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>

<script src="/assets/track-events.js"></script>
<script>trackEvents.bind();</script>


</body>
</html>
