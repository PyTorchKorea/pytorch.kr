<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      RoBERTa | 파이토치 한국 사용자 모임
    
  </title>
  <meta property="og:title" content="PyTorch Korea User Group" />
<meta
  name="description"
  property="og:description"
  content="(Unofficial) Korean user community for PyTorch which is an open source machine learning framework that accelerates the path from research prototyping to production deployment."
/>
<meta property="og:url" content="https://www.pytorch.kr" />
<meta property="og:type" content="website" />
<meta
  property="og:image"
  content="https://pytorch.kr/assets/images/pytorch-kr-logo.png"
/>
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
</head>

  <body class="hub hub-detail">
    <div class="container-fluid header-holder hub-header">
  <div class="container">
    

<div class="header-container">
  <a class="header-logo" href="https://pytorch.kr" aria-label="PyTorchKR"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">시작하기</a>
    </li>

    <li class="main-menu-item">
      <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
    </li>

    <li class="main-menu-item active">
      <a href="/hub">허브</a>
    </li>

    <li class="main-menu-item">
      <a href="https://discuss.pytorch.kr">커뮤니티</a>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

  </div>
</div>


    <div class="main-background hub-background hub-detail-background"></div>

    <div class="jumbotron jumbotron-fluid">
      <div class="container">
        <span class="detail-arrow">
          
            <a href="/hub/research-models"><</a>
          
        </span>
        <h1>
          RoBERTa
        </h1>

        <div class="row">
          <div class="col-md-6">
            <p class="detail-lead">By Facebook AI (fairseq Team) </p>
          </div>

          <div class="col-md-6">
            <p class="detail-lead lead-summary">A Robustly Optimized BERT Pretraining Approach</p>
            <a href="https://github.com/pytorch/fairseq/"><button class="btn btn-lg with-right-white-arrow detail-github-link">View on Github</button></a>
            <a href="https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_roberta.ipynb"><button class="btn btn-lg with-right-white-arrow detail-colab-link">Open on Google Colab</button></a>
          </div>
        </div>
      </div>
    </div>

    <div class="main-content-wrapper">
      <div class="main-content">
        <div class="container">
          <div class="row">
            <div class="col-md-4">
              <img src="/assets/images/" data-image-name="" class="featured-image img-fluid">
              <img src="/assets/images/" data-image-name="" class="featured-image img-fluid">
            </div>
            <div class="col-md-8">
              <article class="pytorch-article">
                <h3 id="model-description">Model Description</h3>

<p>Bidirectional Encoder Representations from Transformers, or <a href="https://arxiv.org/abs/1810.04805">BERT</a>, is a
revolutionary self-supervised pretraining technique that learns to predict
intentionally hidden (masked) sections of text. Crucially, the representations
learned by BERT have been shown to generalize well to downstream tasks, and when
BERT was first released in 2018 it achieved state-of-the-art results on many NLP
benchmark datasets.</p>

<p><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> builds on BERT’s language masking strategy and modifies key
hyperparameters in BERT, including removing BERT’s next-sentence pretraining
objective, and training with much larger mini-batches and learning rates.
RoBERTa was also trained on an order of magnitude more data than BERT, for a
longer amount of time. This allows RoBERTa representations to generalize even
better to downstream tasks compared to BERT.</p>

<h3 id="requirements">Requirements</h3>

<p>We require a few additional Python dependencies for preprocessing:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>regex requests
</code></pre></div></div>

<h3 id="example">Example</h3>

<h5 id="load-roberta">Load RoBERTa</h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">roberta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">hub</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'pytorch/fairseq'</span><span class="p">,</span> <span class="s">'roberta.large'</span><span class="p">)</span>
<span class="n">roberta</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>  <span class="c1"># disable dropout (or leave in train mode to finetune)
</span></code></pre></div></div>

<h5 id="apply-byte-pair-encoding-bpe-to-input-text">Apply Byte-Pair Encoding (BPE) to input text</h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'Hello world!'</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">tokens</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">31414</span><span class="p">,</span> <span class="mi">232</span><span class="p">,</span> <span class="mi">328</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">roberta</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="s">'Hello world!'</span>
</code></pre></div></div>

<h5 id="extract-features-from-roberta">Extract features from RoBERTa</h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract the last layer's features
</span><span class="n">last_layer_features</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">extract_features</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">last_layer_features</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1024</span><span class="p">])</span>

<span class="c1"># Extract all layer's features (layer 0 is the embedding layer)
</span><span class="n">all_layers</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">extract_features</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">return_all_hiddens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">25</span>
<span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="nb">all</span><span class="p">(</span><span class="n">all_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">last_layer_features</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="use-roberta-for-sentence-pair-classification-tasks">Use RoBERTa for sentence-pair classification tasks</h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Download RoBERTa already finetuned for MNLI
</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">hub</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'pytorch/fairseq'</span><span class="p">,</span> <span class="s">'roberta.large.mnli'</span><span class="p">)</span>
<span class="n">roberta</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>  <span class="c1"># disable dropout for evaluation
</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># Encode a pair of sentences and make a prediction
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'Roberta is a heavily optimized version of BERT.'</span><span class="p">,</span> <span class="s">'Roberta is not very optimized.'</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">'mnli'</span><span class="p">,</span> <span class="n">tokens</span><span class="p">).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># contradiction
</span>
    <span class="c1"># Encode another pair of sentences
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'Roberta is a heavily optimized version of BERT.'</span><span class="p">,</span> <span class="s">'Roberta is based on BERT.'</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">'mnli'</span><span class="p">,</span> <span class="n">tokens</span><span class="p">).</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">2</span>  <span class="c1"># entailment
</span></code></pre></div></div>

<h5 id="register-a-new-randomly-initialized-classification-head">Register a new (randomly initialized) classification head</h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">roberta</span><span class="p">.</span><span class="n">register_classification_head</span><span class="p">(</span><span class="s">'new_task'</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">roberta</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="s">'new_task'</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>  <span class="c1"># tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</span></code></pre></div></div>

<h3 id="references">References</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
  <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
</ul>


                <a href=""><button class="btn btn-lg hub-feedback-button hub-flag">Not Working?</button></a>
              </article>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>공식 문서 (영어)</h2>
        <p>PyTorch 공식 문서입니다.</p>
        <a class="with-right-arrow" href="https://pytorch.org/docs" target="_blank">공식 문서로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 PyTorch 튜토리얼입니다.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>커뮤니티</h2>
        <p>다른 사용자들과 의견을 나눠보세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch 홈페이지 (공식)</a></li>
          <li><a href="https://pytorch.org" target="_blank">공식 홈페이지</a></li>
          <li><a href="https://pytorch.org/tutorials" target="_blank">공식 튜토리얼</a></li>
          <li><a href="https://pytorch.org/docs" target="_blank">공식 문서</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="">한국 사용자 모임</a></li>
          <li><a href="/about">사이트 소개</a></li>
          <li><a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a></li>
          <li><a href="https://github.com/9bow/PyTorch-tutorials-kr" target="_blank">한국어 튜토리얼 저장소</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 PyTorch 한국어 사용자 커뮤니티로 Facebook, Inc와 관련이 없습니다. PyTorch, PyTorch 로고 및 모든 관련 표기는 Facebook, Inc의 상표입니다.</li>
        <li>This site is a user community and is not related with Facebook, Inc. PyTorch, the PyTorch logo and any related marks are trademarks of Facebook, Inc.</li>
      </ul>
    </div>
  </div>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156349638-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-156349638-1');
</script>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.kr" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>

        <li class="main-menu-item ">
          <a href="/get-started">시작하기</a>
        </li>

        <li class="main-menu-item">
          <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
        </li>

        <li class="main-menu-item">
          <a href="/hub">허브</a>
        </li>

        <li class="main-menu-item">
          <a href="https://discuss.pytorch.kr">커뮤니티</a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>
<!-- 
  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script> -->

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>

<script src="/assets/track-events.js"></script>
<script>trackEvents.bind();</script>


  </body>
</html>

<script src="/assets/hub-detail.js"></script>
