{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cb8114",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# RoBERTa\n",
    "\n",
    "*Author: Facebook AI (fairseq Team)*\n",
    "\n",
    "**BERT를 강력하게 최적화하는 사전 학습 접근법, RoBERTa**\n",
    "\n",
    "\n",
    "\n",
    "### 모델 설명\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers, [BERT][1]는 텍스트에서 의도적으로 숨겨진 부분을 예측하는 뛰어난 자기지도 사전 학습(self-supervised pretraining) 기술입니다. 특히 BERT가 학습한 표현은 다운스트림 태스크(downstream tasks)에 잘 일반화되는 것으로 나타났으며, BERT가 처음 공개된 2018년에 수많은 자연어처리 벤치마크 데이터셋에 대해 가장 좋은 성능을 기록했습니다.\n",
    "\n",
    "[RoBERTa][2]는 BERT의 언어 마스킹 전략(language masking strategy)에 기반하지만 몇 가지 차이점이 존재합니다. 다음 문장 사전 학습(next-sentence pretraining objective)을 제거하고 훨씬 더 큰 미니 배치와 학습 속도로 훈련하는 등 주요 하이퍼파라미터를 수정합니다. 또한 RoBERTa는 더 오랜 시간 동안 BERT보다 훨씬 많은 데이터에 대해 학습되었습니다. 이를 통해 RoBERTa의 표현은 BERT보다 다운스트림 태스크에 더 잘 일반화될 수 있습니다.\n",
    "\n",
    "\n",
    "### 요구 사항\n",
    "\n",
    "추가적인 Python 의존성이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install regex requests hydra-core omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96258f82",
   "metadata": {},
   "source": [
    "### 예시\n",
    "\n",
    "##### RoBERTa 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval()  # 드롭아웃 비활성화 (또는 학습 모드 비활성화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b1a2c",
   "metadata": {},
   "source": [
    "##### 입력 텍스트에 Byte-Pair Encoding (BPE) 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = roberta.encode('Hello world!')\n",
    "assert tokens.tolist() == [0, 31414, 232, 328, 2]\n",
    "assert roberta.decode(tokens) == 'Hello world!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5b437",
   "metadata": {},
   "source": [
    "##### RoBERTa에서 특징(feature) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547eb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 계층의 특징 추출\n",
    "last_layer_features = roberta.extract_features(tokens)\n",
    "assert last_layer_features.size() == torch.Size([1, 5, 1024])\n",
    "\n",
    "# 모든 계층의 특징 추출\n",
    "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)\n",
    "assert len(all_layers) == 25\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ad1d4",
   "metadata": {},
   "source": [
    "##### 문장 관계 분류(sentence-pair classification) 태스크에 RoBERTa 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNLI에 대해 미세조정된 RoBERTa 다운로드\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "roberta.eval()  # 평가를 위해 드롭아웃 비활성화\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 한 쌍의 문장을 인코딩하고 예측\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    assert prediction == 0  # contradiction\n",
    "\n",
    "    # 다른 문장 쌍을 인코딩하고 예측\n",
    "    tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')\n",
    "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "    assert prediction == 2  # entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61331af1",
   "metadata": {},
   "source": [
    "##### 새로운 분류층 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.register_classification_head('new_task', num_classes=3)\n",
    "logprobs = roberta.predict('new_task', tokens)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f3b1e",
   "metadata": {},
   "source": [
    "### 참고\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding][1]\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach][2]\n",
    "\n",
    "\n",
    "[1]: https://arxiv.org/abs/1810.04805\n",
    "[2]: https://arxiv.org/abs/1907.11692"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
