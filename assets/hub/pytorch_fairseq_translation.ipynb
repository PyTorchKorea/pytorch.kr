{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c430c616",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# Transformer (NMT)\n",
    "\n",
    "*Author: Facebook AI (fairseq Team)*\n",
    "\n",
    "**영어-프랑스어 번역과 영어-독일어 번역을 위한 트랜스포머 모델**\n",
    "\n",
    "\n",
    "\n",
    "### 모델 설명\n",
    "\n",
    "논문 [Attention Is All You Need][1]에 소개되었던 트랜스포머(Transformer)는  \n",
    "강력한 시퀀스-투-시퀀스 모델링 아키텍처로 최신 기계 신경망 번역 시스템을 가능하게 합니다.\n",
    "\n",
    "최근, `fairseq`팀은 역번역된 데이터를 활용한 \n",
    "트랜스포머의 대규모 준지도 학습을 통해 번역 수준을 기존보다 향상시켰습니다.\n",
    "더 자세한 내용은 [블로그 포스트][2]를 통해 찾으실 수 있습니다.\n",
    "\n",
    "\n",
    "### 요구사항\n",
    "\n",
    "전처리 과정을 위해 몇 가지 python 라이브러리가 필요합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9985be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install bitarray fastBPE hydra-core omegaconf regex requests sacremoses subword_nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51963038",
   "metadata": {},
   "source": [
    "### 영어 ➡️ 프랑스어 번역\n",
    "\n",
    "영어를 프랑스어로 번역하기 위해 [Scaling\n",
    "Neural Machine Translation][3] 논문의 모델을 활용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# WMT'14 data에서 학습된 영어 ➡️ 프랑스어 트랜스포머 모델 불러오기:\n",
    "en2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "# GPU 사용 (선택사항):\n",
    "en2fr.cuda()\n",
    "\n",
    "# beam search를 통한 번역:\n",
    "fr = en2fr.translate('Hello world!', beam=5)\n",
    "assert fr == 'Bonjour à tous !'\n",
    "\n",
    "# 토큰화:\n",
    "en_toks = en2fr.tokenize('Hello world!')\n",
    "assert en_toks == 'Hello world !'\n",
    "\n",
    "# BPE 적용:\n",
    "en_bpe = en2fr.apply_bpe(en_toks)\n",
    "assert en_bpe == 'H@@ ello world !'\n",
    "\n",
    "# 이진화:\n",
    "en_bin = en2fr.binarize(en_bpe)\n",
    "assert en_bin.tolist() == [329, 14044, 682, 812, 2]\n",
    "\n",
    "# top-k sampling을 통해 다섯 번역 사례 생성:\n",
    "fr_bin = en2fr.generate(en_bin, beam=5, sampling=True, sampling_topk=20)\n",
    "assert len(fr_bin) == 5\n",
    "\n",
    "# 예시중 하나를 문자열로 변환하고 비토큰화\n",
    "fr_sample = fr_bin[0]['tokens']\n",
    "fr_bpe = en2fr.string(fr_sample)\n",
    "fr_toks = en2fr.remove_bpe(fr_bpe)\n",
    "fr = en2fr.detokenize(fr_toks)\n",
    "assert fr == en2fr.decode(fr_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40757b",
   "metadata": {},
   "source": [
    "### 영어 ➡️ 독일어 번역\n",
    "\n",
    "역번역에 대한 준지도학습은 번역 시스템을 향상시키는데 효율적인 방법입니다.\n",
    "논문 [Understanding Back-Translation at Scale][4]에서,\n",
    "추가적인 학습 데이터로 사용하기 위해 2억개 이상의 독일어 문장을 역번역합니다. 이 다섯 모델들의 앙상블은 [WMT'18 English-German news translation competition][5]의 수상작입니다.\n",
    "\n",
    "[noisy-channel reranking][6]을 통해 이 접근법을 더 향상시킬 수 있습니다. \n",
    "더 자세한 내용은 [블로그 포스트][7]에서 볼 수 있습니다. \n",
    "이러한 노하우로 학습된 모델들의 앙상블은 [WMT'19 English-German news\n",
    "translation competition][8]의 수상작입니다.\n",
    "\n",
    "앞서 소개된 대회 수상 모델 중 하나를 사용하여 영어를 독일어로 번역해보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1792e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# WMT'19 data에서 학습된 영어 ➡️ 독일어 트랜스포머 모델 불러오기:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "# 기본 트랜스포머 모델에 접근\n",
    "assert isinstance(en2de.models[0], torch.nn.Module)\n",
    "\n",
    "# 영어 ➡️ 독일어 번역\n",
    "de = en2de.translate('PyTorch Hub is a pre-trained model repository designed to facilitate research reproducibility.')\n",
    "assert de == 'PyTorch Hub ist ein vorgefertigtes Modell-Repository, das die Reproduzierbarkeit der Forschung erleichtern soll.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f222f2",
   "metadata": {},
   "source": [
    "교차번역으로 같은 문장에 대한 의역을 만들 수도 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 ↔️ 독일어 교차번역:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "de2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "paraphrase = de2en.translate(en2de.translate('PyTorch Hub is an awesome interface!'))\n",
    "assert paraphrase == 'PyTorch Hub is a fantastic interface!'\n",
    "\n",
    "# 영어 ↔️ 러시아어 교차번역과 비교:\n",
    "en2ru = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-ru.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "ru2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.ru-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "\n",
    "paraphrase = ru2en.translate(en2ru.translate('PyTorch Hub is an awesome interface!'))\n",
    "assert paraphrase == 'PyTorch is a great interface!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fda54",
   "metadata": {},
   "source": [
    "### 참고 문헌\n",
    "\n",
    "- [Attention Is All You Need][1]\n",
    "- [Scaling Neural Machine Translation][3]\n",
    "- [Understanding Back-Translation at Scale][4]\n",
    "- [Facebook FAIR's WMT19 News Translation Task Submission][6]\n",
    "\n",
    "\n",
    "[1]: https://arxiv.org/abs/1706.03762\n",
    "[2]: https://code.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/\n",
    "[3]: https://arxiv.org/abs/1806.00187\n",
    "[4]: https://arxiv.org/abs/1808.09381\n",
    "[5]: http://www.statmt.org/wmt18/translation-task.html\n",
    "[6]: https://arxiv.org/abs/1907.06616\n",
    "[7]: https://ai.facebook.com/blog/facebook-leads-wmt-translation-competition/\n",
    "[8]: http://www.statmt.org/wmt19/translation-task.html"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
